[
  {
    "objectID": "posts/statsmodels/Statsmodels.html",
    "href": "posts/statsmodels/Statsmodels.html",
    "title": "Guide to Statsmodels",
    "section": "",
    "text": "This is a guide on the statsmodels module in python. Statsmodels is a module meant for statistical analysis. It has a large repository of different statistical models, datasets, and tools to generate information on data. We will go over a quick guide on ordinary linear regression using statsmodels to introduce the module.\nWe will start by importing the necessary modules.\n\npip install statsmodels\n\nNext, we will import the necessary packages. Numpy will be used to generate the random data needed.\n\ntry:\n    import statsmodels.api as sm\n    import numpy as np\nexcept ImportError as err:\n    print(err)\n\nWe set a random seed for reproducability. Then, we created two 100 term arrays of numbers from 1 to 100 to generate data.\n\nnp.random.seed(999)\nx = np.arange(1, 101)\ny = np.random.randint(1, 100, size=100)\n\nHere is the statsmodels portion. For a linear regression, we will use .add_constant to add the constant term to the linear equation. Without this, statsmodels will have the formula y=mx isntead of y=mx+b. Next, we will use call OLS (ordinary linear regression) on the data and fit it.\n\nx = sm.add_constant(x)\nlin_reg = sm.OLS(y, x).fit()\n\nWe can use the summary method on the model to see all of the information we need. Note that the constant also has values.\n\nlin_reg.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.028\n\n\nModel:\nOLS\nAdj. R-squared:\n0.018\n\n\nMethod:\nLeast Squares\nF-statistic:\n2.803\n\n\nDate:\nSun, 08 Dec 2024\nProb (F-statistic):\n0.0973\n\n\nTime:\n20:23:14\nLog-Likelihood:\n-470.72\n\n\nNo. Observations:\n100\nAIC:\n945.4\n\n\nDf Residuals:\n98\nBIC:\n950.7\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n56.5376\n5.455\n10.365\n0.000\n45.713\n67.362\n\n\nx1\n-0.1570\n0.094\n-1.674\n0.097\n-0.343\n0.029\n\n\n\n\n\n\n\n\nOmnibus:\n18.182\nDurbin-Watson:\n1.879\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n4.999\n\n\nSkew:\n-0.144\nProb(JB):\n0.0821\n\n\nKurtosis:\n1.943\nCond. No.\n117.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWe can also print individual the values that we want. We use [1] to grab the information for our x values (x1). [0] will give the value of the constant. Leaving out the index will give a list of values. Only properties that differ between inputs need indexing, as seen with rsquared.\n\nprint(lin_reg.pvalues[1])\nprint(lin_reg.params[1])\nprint(lin_reg.rsquared)\n\n0.09730520219754973\n-0.15698169816981677\n0.02780206533271945\n\n\nLook at the printed values and the summary and you should be able to see what params is. Everything we want is in the summary and can be grabbed directly from the model. We will use these values to make predictions with a linear model and the standard y=mx+b formula with x values 1 to 100. x0 is created because x is now a 2D array with added constants.\n\nb = lin_reg.params[0]\nm = lin_reg.params[1]\nx0 = np.arange(1, 101)\nyhat = m * x0 + b\nyhat\n\narray([56.38059406, 56.22361236, 56.06663066, 55.90964896, 55.75266727,\n       55.59568557, 55.43870387, 55.28172217, 55.12474047, 54.96775878,\n       54.81077708, 54.65379538, 54.49681368, 54.33983198, 54.18285029,\n       54.02586859, 53.86888689, 53.71190519, 53.55492349, 53.39794179,\n       53.2409601 , 53.0839784 , 52.9269967 , 52.770015  , 52.6130333 ,\n       52.45605161, 52.29906991, 52.14208821, 51.98510651, 51.82812481,\n       51.67114311, 51.51416142, 51.35717972, 51.20019802, 51.04321632,\n       50.88623462, 50.72925293, 50.57227123, 50.41528953, 50.25830783,\n       50.10132613, 49.94434443, 49.78736274, 49.63038104, 49.47339934,\n       49.31641764, 49.15943594, 49.00245425, 48.84547255, 48.68849085,\n       48.53150915, 48.37452745, 48.21754575, 48.06056406, 47.90358236,\n       47.74660066, 47.58961896, 47.43263726, 47.27565557, 47.11867387,\n       46.96169217, 46.80471047, 46.64772877, 46.49074707, 46.33376538,\n       46.17678368, 46.01980198, 45.86282028, 45.70583858, 45.54885689,\n       45.39187519, 45.23489349, 45.07791179, 44.92093009, 44.76394839,\n       44.6069667 , 44.449985  , 44.2930033 , 44.1360216 , 43.9790399 ,\n       43.82205821, 43.66507651, 43.50809481, 43.35111311, 43.19413141,\n       43.03714971, 42.88016802, 42.72318632, 42.56620462, 42.40922292,\n       42.25224122, 42.09525953, 41.93827783, 41.78129613, 41.62431443,\n       41.46733273, 41.31035104, 41.15336934, 40.99638764, 40.83940594])\n\n\nWe actually do not have to manually predict our values. Statsmodels has a method for that using .predict().\n\nyhat2 = lin_reg.predict(x)\nyhat2\n\narray([56.38059406, 56.22361236, 56.06663066, 55.90964896, 55.75266727,\n       55.59568557, 55.43870387, 55.28172217, 55.12474047, 54.96775878,\n       54.81077708, 54.65379538, 54.49681368, 54.33983198, 54.18285029,\n       54.02586859, 53.86888689, 53.71190519, 53.55492349, 53.39794179,\n       53.2409601 , 53.0839784 , 52.9269967 , 52.770015  , 52.6130333 ,\n       52.45605161, 52.29906991, 52.14208821, 51.98510651, 51.82812481,\n       51.67114311, 51.51416142, 51.35717972, 51.20019802, 51.04321632,\n       50.88623462, 50.72925293, 50.57227123, 50.41528953, 50.25830783,\n       50.10132613, 49.94434443, 49.78736274, 49.63038104, 49.47339934,\n       49.31641764, 49.15943594, 49.00245425, 48.84547255, 48.68849085,\n       48.53150915, 48.37452745, 48.21754575, 48.06056406, 47.90358236,\n       47.74660066, 47.58961896, 47.43263726, 47.27565557, 47.11867387,\n       46.96169217, 46.80471047, 46.64772877, 46.49074707, 46.33376538,\n       46.17678368, 46.01980198, 45.86282028, 45.70583858, 45.54885689,\n       45.39187519, 45.23489349, 45.07791179, 44.92093009, 44.76394839,\n       44.6069667 , 44.449985  , 44.2930033 , 44.1360216 , 43.9790399 ,\n       43.82205821, 43.66507651, 43.50809481, 43.35111311, 43.19413141,\n       43.03714971, 42.88016802, 42.72318632, 42.56620462, 42.40922292,\n       42.25224122, 42.09525953, 41.93827783, 41.78129613, 41.62431443,\n       41.46733273, 41.31035104, 41.15336934, 40.99638764, 40.83940594])\n\n\nNotice any differences between the arrays? We can check!\n\nnp.array_equal(yhat, yhat2)\n\nTrue\n\n\nNumpy confirms that the two methods are equal. You will likely use the predict method from now on, but it is good to know both. That is the end of the linear regression. Linear regression is only a small part of the statsmodels toolkit. Statsmodels even has ways to change the formula used in regression. To further explore statsmodels, you can visit their website here!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "dat303-blog",
    "section": "",
    "text": "Guide to Statsmodels\n\n\n\n\n\n\nPython\n\n\n\nA guide on the basics of the statsmodels Python module.\n\n\n\n\n\nDec 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT API Guide\n\n\n\n\n\n\nPython\n\n\n\nA guide on using the ChatGPT API.\n\n\n\n\n\nNov 28, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/chatgpt-api/chatgpt-api.html",
    "href": "posts/chatgpt-api/chatgpt-api.html",
    "title": "ChatGPT API Guide",
    "section": "",
    "text": "This is a guide on how to access OpenAI’s API using python and the openai module. This guide will go over how to create an OpenAI account, how to get an API key, and how to use said key with the API.\n\nStep 1: Making an Account\nStart by going to https://platform.openai.com. In the top right corner, click the green “Sign Up” button. Either use your email to sign up or use one of the options below and follow the steps given to create your account.\n\n\n\nplatform.openai home page\n\n\n\n\nStep 2: Creating Your API Key\nTo access the OpenAI API, you need an OpenAI API key. You can the API Keys page by going to your profile from your account icon in the upper right corner. From there, select API Keys on the left sidebar. You should be on this page.\n\n\n\nAPI Keys Webpage\n\n\nYou can see my current API keys in the image. To create a new API key, click the green “Create new secret key” button in the upper right corner. Name the key whatever you want and select “Defualt project” from the Project dropdown. Then create the key.\n\n\n\nAPI Key Creation Window\n\n\nOnce the key is created, save it somewhere safe, but close by. You will never be able to access it from the website again, and we will need it to access the API.\n\n\nStep 2.5: Buying Credits\nIn order to use the API, you will need to buy credits. Each call to the API will consume a very small amount of credits until you run out and the API key stops working. To being using the API, we will need to buy $5 worth of credits. From the same page as before, select Billing on the left sidebar. You should then be on the screen below.\n\n\n\nAPI Credit Page\n\n\nSelect “Add payment details” and selevt Individual. Input your payment information. Change the initial purchase amount to $5. Uncheck the automatic recharge option and click continue. Confirm the payment. These credits will give you token that will be consumed every time you make a call to the API, so you must keep your key safe to prevent unwanted usage.\n\n\nStep 3: Setting Up the API Code\nNow, it is time to start accessing the API using python. In your IDE of choice, install the openai module using the terminal.\n\npip install openai\n\nNext, import the openai and os modules.\n\ntry:\n    from openai import OpenAI\n    import os\nexcept ImportError as err:\n    print(err)\n\nUsing the API key you created, set the environment variable.\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY_HERE\"\n\nFinally, initialize the chatbot.\n\nclient = OpenAI(\n    api_key=os.environ.get(\"OPEN_API_KEY\")\n)\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"This is an example.\",\n        }\n    ],\n    model=\"gpt-4o\",\n)\nresponse = chat_completion.choices[0].message.content\nprint(response)\n\nGot it! If you have any questions or need help with something specific, feel free to let me know.\n\n\nFirst, we set the client using the api_key to gain access. Then, we set the content value in the messages list to “This is an example.” The exact response given will differ each time, but you can see that the value of the content key is equivalent to the prompt given to ChatGPt. The model variable is the name of a ChatGPT model to tell it which one to use. The higher level the model, the more tokens it will consume with each use. To learn more about all of the available models, you can go to https://platform.openai.com/docs/models. The “.choices[0].message.content” at the end will access specifically the response. The rest will likely not be relevant for you, but you can experiment with removing parts from the end to look at the rest of the output.\nWe can also use the input() function to give custom prompts. Because we have already set the API key, we will skip that step this time.\n\nprompt = input()\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": prompt,\n        }\n    ],\n    model=\"gpt-4o\",\n)\nresponse = chat_completion.choices[0].message.content\nprint(response)\n\nWhy don't skeletons fight each other? \n\nThey don't have the guts.\n\n\nIn this example, I gave the prompt “Tell me a joke” using the input function and recieved a joke in response. You can put whatever you want for your own needs. The output will be a string that can used."
  }
]